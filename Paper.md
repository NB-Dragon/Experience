# 1 INTRODUCTION
&ensp;&ensp;Selection and compositing are at the core of the image editing process. For instance, local adjustments often start with a selection, and combining elements from different images is a powerful way to produce new content. But creating an accurate selection is a tedious task especially when fuzzy boundaries and transparency are involved. Tools such as the magnetic lasso and the magic wand exist to assist users but they only exploit low-level cues and heavily rely on the users’ skills and interpretation of the image content to produce good results. Furthermore, they only produce binary selections that need further refinement to account for soft boundaries like the silhouette of a furry dog. Matting tools also exist to help users with this task but they only add to the tedium of the entire editing process.  
&ensp;&ensp;An accurate pre-segmentation of the image can speed up the editing process by providing an intermediate image representation if it satisfies several criteria. First of all, such a segmentation should provide distinct segments of the image, while also representing the soft transitions between them accurately. In order to allow targeted edits, each segment should be limited to the extent of a semantically meaningful region in the image, e.g., it should not extend across the boundary between two objects. Finally, the segmentation should be done fully automatically not to add a point of interaction or require expertise from the artist. The previous approaches for semantic segmentation, image matting, or soft color segmentation fail to satisfy at least one of these qualities. In this paper, we introduce semantic soft segmentation, a fully automatic decomposition of an input image into a set of layers that cover scene objects, separated by soft transitions.  
&ensp;&ensp;We approach the semantic soft segmentation problem from a spectral decomposition angle. We combine the texture and color information from the input image together with high-level semantic cues that we generate using a convolutional neural network trained for scene analysis. We design a graph structure that reveals the semantic objects as well as the soft transitions between them in the eigenvectors of the corresponding Laplacian matrix. We introduce a spatially varying model of layer sparsity that generates high-quality layers from the eigenvectors that can be utilized for image editing.  
&ensp;&ensp;We demonstrate that our algorithm successfully decomposes images into a small number of layers that compactly and accurately represent the scene objects as shown in Figure 1. We later show that our algorithm can successfully process images that are challenging for other techniques and we provide examples of editing operations such as local color adjustment or background replacement that benefit from our layer representation.  
# 2 RELATED WORK
&ensp;&ensp;Soft segmentation. Soft segmentation is decomposing an image into two or more segments where each pixel may belong partially to more than one segment. The layer contents change depending on the specific goal of the corresponding method. For instance, soft color segmentation methods extract soft layers of homogeneous colors using global optimization [Singaraju and Vidal 2011; Tai et al. 2007; Tan et al. 2016] or per-pixel color unmixing [Aksoy et al. 2016, 2017b]. While soft color segments are shown to be useful for several image editing applications such as image recoloring, their content typically does not respect object boundaries, not allowing targeted edits. To generate spatially connected soft segments, Singaraju and Vidal [2011] start from a set of user-defined regions and solve two-layer soft segmentation problems multiple times to generate multiple layers. Levin et al. [2008b], on the other hand, propose spectral matting, estimating a set of spatially connected soft segments automatically via spectral decomposition. Both Singaraju and Vidal [2011] and Levin et al. [2008b] construct their algorithms around the matting Laplacian [Levin et al. 2008a], which provides a powerful representation for local soft transitions in the image. We also make use of the matting Laplacian and spectral decomposition, following ideas from spectral matting. However, unlike previous work, we construct a graph that fuses higher-level information coming from a deep network with the local texture information in order to generate soft segments that correspond to semantically meaningful regions in the image.  
&ensp;&ensp;Natural image matting. Natural image matting is the estimation of per-pixel opacities of a user-defined foreground region. The typical input to natural matting algorithms is a trimap, which defines the opaque foreground, the transparent background, and the unknownopacity regions. While there are different approaches to this problem all of which make use of the color characteristics of the defined foreground and background regions, the most closely-related approaches to ours are categorized as affinity-based methods. The affinity-based methods, such as closed-form matting [Levin et al. 2008a], KNN matting [Chen et al. 2013], and information-flow matting [Aksoy et al. 2017a], define inter-pixel affinities to construct a graph that reflects the opacity transitions in the image. In contrary to natural image matting methods, we rely on automatically-generated semantic features in defining our soft segments instead of a trimap, and generate multiple soft segments rather than foreground segmentation. Although they appear similar, natural matting and soft segmentation have fundamental differences. Natural matting, with a trimap as input, becomes the problem of foreground and background color modeling, may it be through selection of color samples or propagation of color information. Meanwhile, soft segmentation focuses on detecting soft transitions that best serve the target application, in our case the ones corresponding to semantic boundaries.  
&ensp;&ensp;Targeted edit propagation. Several image editing methods rely on user-defined sparse edits on the image and propagate them to the whole image. ScribbleBoost [Li et al. 2008] proposed a pipeline where they classify the objects specified by the user scribbles to allow edits targeting specific object classes in the image, and DeepProp [Endo et al. 2016] utilized a deep network to propagate class-dependent color edits. Eynard et al. [2014] constructs a graph and, parallel to our method, analyze the eigendecomposition of the corresponding Laplacian matrix to create coherent recoloring results. An and Pellacini [2008] and Chen et al. [2012] also define inter-pixel affinities and make use of the properties of the Laplacian matrices to solve for plausible propagations of the user-defined edits. While our results can also be used for targeted edits, rather than using edits defined a priori, we directly decompose the image into soft segments and let the artist use them as an intermediate image representation in various scenarios and using external image editing tools.  
&ensp;&ensp;Semantic segmentation. Semantic segmentation has improved significantly with the introduction of deep neural networks. While a detailed report on semantic segmentation is beyond our scope, state-of-the-art in semantic segmentation include works on scene parsing by Zhao et al. [2017], instance segmentation methods by He et al. [2017] and Fathi et al. [2017], and work by Bertasius et al. [2016] which enhances semantic segmentation with color boundary cues. We also make use of a deep network for semantic features, but our soft segmentation method is class-agnostic, i.e. we are interested in an accurate segmentation of the image respecting semantic boundaries, but we do not aim to do classification or detection of a selected set of classes. Others also make use of class-agnostic semantic information to improve performance in video deblurring [Ren et al. 2017] or cinemagraph generation [Oh et al. 2017].
# 3 METHOD
&ensp;&ensp;We seek to automatically generate a soft segmentation of the input image, that is, a decomposition into layers that represent the objects in the scene including transparency and soft transitions when they exist. Each pixel of each layer is augmented with an opacity value α ∈ [0, 1] with α = 0 meaning fully transparent, α = 1 fully opaque , and in-between values indicating the degree of partial opacity. As other studies in this domain such as [Aksoy et al. 2017b; Singaraju and Vidal 2011], we use an additive image formation model:{formula}, i.e., we express the input RGB pixels as the sum of the pixels in each layer i weighted by its corresponding α value. We also constrain the α values to sum up to 1 at each pixel, representing a fully opaque input image.  
&ensp;&ensp;Our approach uses the same formalism as spectral matting in formulating the soft segmentation task as an eigenvector estimation problem [Levin et al. 2008b]. The core component of this approach is the creation of a Laplacian matrix L that represents how likely each pair of pixels in the image is to belong to the same segment.  
&ensp;&ensp;While spectral matting builds this matrix using only low-level local color distributions, we describe how to augment this approach with nonlocal cues and high-level semantic information. The original approach also describes how to create the layers from the eigenvec tors of L using sparsification. We show how a relaxed version of this original technique actually yields better results. Figure 2 shows an overview of our approach.  
## 3.1 Background
&ensp;&ensp;Spectral matting. Our approach builds upon the work of Levin et al. [2008a; 2008b]. They first introduced the matting Laplacian that uses local color distributions to define a matrix L that captures the affinity between each pair of pixels in a local patch, typically 5 × 5 pixels. Using this matrix, they minimize the quadratic functional α T Lα subject to user-provided constraints, with α denoting a vector made of all the α values for a layer. This formulation shows that the eigenvectors associated to small eigenvalues of L play an important role in the creation of high-quality mattes. Motivated by this observation, their subsequent work on spectral matting used the eigenvectors of L to build a soft segmentation [Levin et al. 2008b].  
&ensp;&ensp;Each soft segment is a linear combination of the K eigenvectors corresponding to the smallest eigenvalues of L that maximizes matting sparsity, i.e., minimizes the occurrence of partial opacity. The segments are created by minimizing an energy function that favors α = 0 and α = 1:{formula}, where α ip is the α value of p th pixel of the i th segment, E is a matrix containing the K eigenvectors of L with smallest eigenvalues, y i is the linear weights on the eigenvectors that define the soft segments, and γ < 1 is a parameter that controls the strength of the sparsity prior.  
&ensp;&ensp;While spectral matting generates satisfying results when the image contains a single well-identified object with distinct colors, it struggles with more complex objects and scenes. Being based solely on the matting Laplacian that considers only low-level statistics of small patches, it is limited in its ability to identify objects. In our work, we extend this approach to fuse semantic features in the same Laplacian formulation and capture higher-level concepts like scene objects and to have a broader view of the image data.  
&ensp;&ensp;Affinity and Laplacian matrices. Levin et al. [2008a] formulate their approach as a least-squares optimization problem that directly leads to a Laplacian matrix. An alternative approach is to express the affinity between pairs of pixels [Aksoy et al. 2017a]. Pairs with a positive affinity are more likely to have similar values, zero-affinity pairs are independent, and pairs with a negative affinity are likely to have different values. In this work, we will use the affinity approach and build the corresponding normalized Laplacian matrix using the well-known formula:{formula}, where W is a square matrix containing the affinity between all pairs of pixels and D is the corresponding degree matrix, i.e. a diagonal matrix with elements W 1, 1 being a row vector of ones. As noted by Levin et al., L may not always be a true graph Laplacian due to the presence negative affinities, but nonetheless shares similar properties such as being positive semidefinite.  
## 3.2 Nonlocal Color Affinity
&ensp;&ensp;We define an additional low-level affinity term that represents colorbased longer-range interactions. A naive approach would be to use larger patches in the definition of the matting Laplacian. However, this option quickly becomes impractical because it renders the Laplacian matrix denser. Another option is to sample pixels from a nonlocal neighborhood to insert connection while preserving some sparsity in the matrix. KNN matting [Chen et al. 2013] and information-flow matting [Aksoy et al. 2017a] have shown good results for medium-range interaction with such sampling. However, this strategy faces a trade-off between sparsity and robustness: fewer samples may miss important image features and more samples make the computation less tractable.  
&ensp;&ensp;We propose a guided sampling based on an oversegmentation of the image. We generate 2500 superpixels using SLIC [Achanta et al. 2012] and estimate the affinity between each superpixel and all the superpixels within a radius that corresponds to 20% of the image size.  
&ensp;&ensp;The advantage of this approach is that each feature large enough to be a superpixel is represented, sparsity remains high because we use a single sample per superpixel, and it links possibly disconnected regions by using a large radius, e.g. when the background is seen through a hole in an object. Formally, we define the color affinity w between the centroids of two superpixels s and t separated by a distance less than 20% of the image size as:{formula}, where c s and c t are the average colors of the superpixels of s and t that lies in [0, 1], erf is the Gauss error function, and a c and b c are parameters controlling how quickly the affinity degrades and the threshold where it becomes zero. erf takes values in [−1, 1] and its use here is mainly motivated by its sigmoidal shape. We use a c = 50 and b c = 0.05 in all our results. This affinity essentially makes sure the regions with very similar colors stay connected in challenging scene structures, and its effect is demonstrated in Figure 3.  
## 3.3 High-Level Semantic Affinity
&ensp;&ensp;While the nonlocal color affinity adds long-range interactions to the segmentation process, it remains a low-level feature. Our experiments show that, without additional information, the segmentation still often merges image regions of similar color that belong to different objects. To create segments that are confined in semantically similar regions, we add a semantic affinity term, that is, a term that encourages the grouping of pixels that belong to the same scene object and discourages that of pixels from different objects. We build upon prior work in the domain of object recognition to compute a feature vector at each pixel that correlates with the underlying object. We compute the feature vectors via a neural network, as described in Section 3.5. The feature vectors are generated such that for two pixels p and q that belong to the same object f p and f q are similar, i.e. ∥ f p − f q ∥ ≡ 0, and for a third pixel r in a different semantic region, f r is far away, i.e. ∥ f p − f q ∥ ≪ ∥ f p − f r ∥.  
&ensp;&ensp;We define the semantic affinity also over superpixels. In addition to increasing the sparsity of the linear system, the use of superpixels also decrease the negative effect of the unreliable feature vectors in transition regions, as apparent from their blurred appearance in Figure 4. The superpixel edges are not directly used in the linear system, the connections in the graph are between superpixel centroids. This information from the centroids then spreads to nearby pixels while respecting the image edges with the matting affinity term. With these vectors and the same oversegmentation in the previous section (§ 3.2), for each superpixel s, we associate its average feature vector f&ensp;&ensp;̃ s to its centroid p s . We use these vectors to define an affinity term between each adjacent superpixels s and t:{formula}, with a s and b s parameters controlling the steepness of the affinity function and when it becomes negative. We discuss how to set them in Section 3.5. Defining negative affinities help the graph disconnect different objects while the positive values connect regions that belong to the same object.  
&ensp;&ensp;Unlike the color affinity, the semantic affinity only relates nearby superpixels to favor the creation of connected objects. This choice of a nonlocal color affinity together with a local semantic affinity allows creating layers that can cover spatially disconnected regions of the same semantically coherent region. This often applies to elements like greenery and sky that often appear in the background, which makes them likely to be split into several disconnected components due to occlusions. As a result of including the local semantic affinity, the eigenvectors of L reveal object boundaries as demonstrated in Figure 4 and 5.  
## 3.4 Creating the Layers
&ensp;&ensp;We create the layers by using the affinities described earlier in this section to form a Laplacian matrix L. We extract the eigenvectors from this matrix and use a two-step sparsification process to create the layers from these eigenvectors.  
&ensp;&ensp;Forming the Laplacian matrix. We form a Laplacian matrix L by adding the affinity matrices together and using Equation 3:{formula}, where W L is the matrix containing the matting affinities, W C the matrix containing the nonlocal color affinities (§ 3.2), W S the matrix with the semantic affinities (§ 3.3), and σ S and σ C parameters controlling the influence of each term, both set to be 0.01.  
&ensp;&ensp;Constrained sparsification. We extract the eigenvectors corresponding to the 100 smallest eigenvalues of L. We form an intermediate set of layers using the optimization procedure by Levin et al. [2008b] on Eq. 2 with γ = 0.8. Unlike spectral matting that uses k-means clustering on the eigenvectors to initialize the optimization, we use k-means clustering on the pixels represented by their feature vectors f . This initial guess is more consistent with the scene semantics and yields a better soft segmentation. We generate 40 layers with this approach and in practice, several of them are all zeros, leaving 15 to 25 nontrivial layers. We further reduce the number of layers by running the k-means algorithm with k = 5 on these nontrivial layers represented by their average feature vector.  
&ensp;&ensp;This approach works better than trying to directly sparsify the 100 eigenvectors into 5 layers, because such drastic reduction makes the problem overly constrained and does not produce good-enough results, especially in terms of matte sparsity. The initially estimated soft segments before and after grouping are shown in Figure 7. We have set the number of segments to 5 without loss of generalization; while this number could be set by the user depending on the scene structure, we have observed that it is a reasonable number for most images. Because these 5 layers are constrained to lie within the subspace of a limited number of eigenvectors, the achieved sparsity is suboptimal, leaving many semi-transparent regions in the layers, which is unlikely in common scenes. Next, we introduced a relaxed version of the sparsity procedure to address this issue.  
&ensp;&ensp;Relaxed sparsification. To improve the sparsity of the layers, we relax the constraint that they are a linear combination of the eigenvectors. Instead of working with the coefficients y i of the linear combination (Eq. 2), in this step, each individual α value is an unknown. We define an energy function that promotes matte sparsity on the pixel-level while respecting the initial soft segment estimates from the constrained sparsification and the image structure. We now define our energy term by term.  
&ensp;&ensp;The first term relaxes the subspace constraint and only ensures that the generated layers remain close to the layers α̂ created with the constrained sparsification procedure:{formula}, We also relax the sum-to-one requirement (Eq. 1b) to be integrated into the linear system as a soft constraint:{formula}, where α ip is the α value of the p th pixel in the i th layer. The next term is the energy defined by the Laplacian L defining the spatial propagation of information defined in Eq.6:{formula}, Finally, we formulate a sparsity term that adapts to the image content. Intuitively, partial opacities come from color transitions in the image because in many cases, it corresponds to a transition between two scene elements, e.g., the fuzzy transition between a teddy bear and the background. We use this observation to build a spatially varying sparsity energy:{formula}, where ∇c p is the color gradient in the image at pixel p computed using the separable kernels of Farid and Simoncelli [2004]. We design this term such that when γ̃ p = 1 on image regions where the gradient is large enough, the energy profile is flat for α ip ∈ [0 : 1], i.e. the energy only acts as a penalty on values outside the valid range and lets α ip take any value between 0 and 1. In comparison, in uniform regions where ∇c p ≈ 0, it encourages α ip to be 0 or 1. These two effects combined favor a higher level of sparsity together with the softness of the opacity transitions. The effect of our spatially varying sparsity energy on preserving accurate soft transitions can be seen in Figure 6 (c,d).  
&ensp;&ensp;Putting these terms together, we get the energy function:{formula}, A unit weight for each term works well except for the sum-to-one term E C that represents the soft constraint with a higher weight λ = 100. Without the sparsity term E S , E would be a standard leastsquares energy function that can be minimized by solving a linear system. To handle E S , we resort to an iterative reweighted leastsquares solver that estimates a solution by solving a series of linear systems. We describe the detail of this approach in the rest of this section.  
&ensp;&ensp;We name N i = 5 the number of layers, N p the number of pixels, a the vector made of all α i ’s and â the vector made of all α̂ i ’s.  
&ensp;&ensp;The dimensionality of a and â is N ip = N i N p . For clarity, we also introduce the N ip × N ip identity matrix I. With this notation, we rewrite E F (Eq. 7) in matrix form:{formula}, We included the redundant I in this equation for a clearer transition when deriving Eq. 17. For rewriting E C (Eq. 8), we introduce the N i × N ip matrix C made by concatenating N i identity matrices horizontally, the vector 1 i made of N i ones, and the vector 1 ip made of N ip ones:{formula}, where we used a T C T 1 i = 1 i T Ca, C T 1 i = 1 ip , and 1 i T 1 i = N i . We then rewrite E L :{formula}, For the sparsity term E S , we introduce the approximate energy:{formula}, where α ′ is equal to the constrained sparsification result at the first iteration and to the solution of the previous iteration later. For the matrix reformulation, we use D u the diagonal matrix built with the u ip values, and v and D v the vector and diagonal matrix built with the v ip values:{formula}, where we used D v 1 ip = v and v t a = a t v To derive a linear system, we sum all the energy terms in their matrix forms and write that the derivative with respect to a should be zero at a minimum. This leads to:{formula}, We solve this equation using preconditioned conjugate gradient optimization [Barrett et al. 1994]. In our experiments, 20 iterations generate results with satisfactory sparsity. Figure 6 illustrates the benefits of our approach.  
&ensp;&ensp;The size of the linear system is N i N p . While this is large, it remains tractable because the number of soft layers N i is set to 5 and it is close to being block-diagonal, the only coefficients outside the diagonal coming from the sum-to-one term E C that contributes C T C to the system. Since C is made of 5 juxtaposed N p ×N p identity matrices, C T C is made of 25 N p × N p identity matrices in a 5 × 5 layout, i.e. it is very sparse and is easily handled by the solver.  
## 3.5 Semantic Feature Vectors
&ensp;&ensp;We defined our semantic affinity term (§ 3.3) with feature vectors f that are similar for pixels on the same object and dissimilar for pixels on different objects. Such vectors can be generated using different network architectures trained for semantic segmentation.  
&ensp;&ensp;In our implementation, we have combined a semantic segmentation approach with a network for metric learning. It should be noted that we do not claim the feature generation as a contribution and we only summarize the solution that we used in this section. A detailed description is provided in the supplementary material.  
&ensp;&ensp;The base network of our feature extractor is based on DeepLabResNet-101 [Chen et al. 2017], but it is trained with a metric learning approach [Hoffer and Ailon 2015] to maximize the L2 distance between the features of different objects. We combine features at multiple stages of the network, motivated by Hariharan et al. [2015] and Bertasius et al. [2015], essentially combining the mid-level and high-level features together. Instead of using all the pixels of an image while training, we generate the features for all pixels but use only a set of randomly-sampled features to update the network. The network minimizes the distance between the features of samples having same ground-truth classes, and maximizes the distance otherwise. Since we only use this cue, i.e. whether two pixels belong to the same category or not, specific object category information is not used during training. Hence, our method is a class agnostic approach. This is suitable for our overall goal of semantic soft segmentation as we aim to create soft segments that cover semantic objects, rather than classification of the objects in an image. To utilize more data with computational efficiency, we use a slightly modified version of N-pair loss [Sohn 2016].  
&ensp;&ensp;We train this network on the semantic segmentation task of the COCO-Stuff dataset [Caesar et al. 2016]. We refine the feature map generated by this network to be well-aligned to image edges using the guided filter [He et al. 2013]. We then use principal component analysis (PCA) to reduce the dimensionality to three. These preprocessing steps are visualized in Figure 8. While the original 128dimensional vectors provide a good coverage of all the content we may encounter, each image only exhibits a small portion of it and reducing the dimensionality accordingly results in better accuracy per dimension. Finally, we normalize the vectors to take values in [0, 1]. This makes it easier to set parameters, especially in case of changing feature vector definitions. For all the results we present, we set a s and b s in Eq. 5 to be 20 and 0.2, respectively.  
## 3.6 Implementation Details
&ensp;&ensp;We use the sparse eigendecomposition and direct solver available in MATLAB for our proof-of-concept implementation for the constrained sparsification stage of our algorithm. This step takes around 3 minutes for a 640 × 480 image. The relaxed sparsification step uses the preconditioned conjugate gradient optimization implementation of MATLAB. Each iteration typically converges in 50 to 80 iterations and the process takes around 30 seconds. The run-time of our algorithm grows linearly with the number of pixels.  
# 4 EXPERIMENTAL ANALYSIS
&ensp;&ensp;Semantic soft segmentation, being at the intersection of semantic segmentation, natural image matting, and soft segmentation, is challenging to evaluate numerically. Semantic segmentation datasets provide binary labeling that is not always pixel-accurate, which makes them ill-suited for benchmarking semantic soft segmentation. Natural image matting methods are typically evaluated on dedicated benchmarks [Rhemann et al. 2009] and datasets [Xu et al. 2017]. These benchmarks are designed to evaluate methods that make use of a secondary input, called trimap, which defines the expected foreground and background, and an uncertain region. Further, the semantic aspect of our work is beyond the scope of these benchmarks. Soft color segmentation, on the other hand, is a problem that lacks a solid definition of ground truth. Although Aksoy et al. [2017b] proposed several blind metrics for evaluation, they are specifically designed for soft color segmentation and also ignores semantic aspects. As a result, we resort to qualitative comparisons with related methods and discuss the characteristic differences between the various approaches.  
## 4.1 Spectral Matting and Semantic Segmentation
&ensp;&ensp;In Figures 9 and 10, we show our results together with that of spectral matting [Levin et al. 2008b] as the most related soft segmentation method to ours, and two state-of-the-art methods for semantic segmentation: the scene parsing method by Zhao et al. [2017] (PSPNet) and the instance segmentation method by He et al. [2017] (Mask R-CNN). More of these comparisons are available in the supplementary material. Spectral matting generates around 20 soft segments per image, and provides several alternative foreground mattes by combining the soft segments to maximize an objectness score. These mattes are not definite results but are provided to the user as options, and showing all 20 segments would make the comparisons harder to evaluate. Instead, we apply our soft segment grouping method that uses the semantic features to the results of spectral matting.  
&ensp;&ensp;The presented examples show that semantic segmentation methods, while being successful in recognizing and locating the objects in the image, suffer from low accuracy around the edges of the objects. While their accuracy is satisfactory for the task of the semantic segmentation, errors around object edges are problematic for image editing or compositing applications. On the other end of the spectrum, spectral matting is able to successfully capture most of the soft transitions around the objects. However, due to the lack of semantic information, their segments often cover multiple objects at once, and the alpha values are often not sparse for any given object. In comparison, our method captures objects in their entirety or subparts of them without grouping unrelated objects and achieves a high accuracy at edges, including soft transitions when appropriate.  
&ensp;&ensp;It should be noted that it is not uncommon for our method to represent the same object in multiple segments such as the horse carriage in Figure 9 (2) or the background fence in Figure 9 (4).  
&ensp;&ensp;This is mainly due to the preset number of layers, five, sometimes exceeds the number of meaningful regions in the image. Some small objects may be missed in the final segments despite being detected by the semantic features, such the people in the background in Figure 10 (5). This is due to the fact that, especially when the color of the object is similar to the surroundings, the objects do not appear well-defined in the eigenvectors and they end up being merged into closeby segments. Our semantic features are not instance-aware, i.e., the features of two different objects of the same class are similar.  
&ensp;&ensp;This results in multiple objects being represented in the same layer such as the cows in Figure 9 (1), the people in Figure 9 (5) or the giraffes in Figure 10 (3). With instance-aware features, however, our method would be capable of generating separate soft segments for different instances of objects.  
&ensp;&ensp;Grayscale images are especially challenging for soft segmentation and image matting methods with the lack of color cues on which such methods typically rely. The performance of semantic segmentation methods, on the other hand, does not degrade substantially when processing a grayscale image. Figure 10 (5) demonstrates that our method can succesfully leverage the semantic information for soft segmentation of a grayscale image.  
## 4.2 Natural Image Matting
&ensp;&ensp;In principle, semantic soft segments can be generated by cascading semantic segmentation and natural image matting. The trimap, defining the foreground, background, and soft transition regions, can be generated from the semantic hard segments to be fed to the natural matting method. Shen et al. [2016] and Qin et al. [2017] use similar approaches for class-specific problems. We show two examples of such scenario in Figure 11 to demonstrate the shortcomings of this approach by generating trimaps using Mask R-CNN and PSPNet results and estimating the mattes using a state-of-the-art matting method, information-flow matting [Aksoy et al. 2017a]. A strong assumption made by natural image matting methods is that the provided trimap is correct, i.e. the defined foreground and background regions are used as hard constraints to guide the methods in modeling the layer colors. Inaccuracies in the estimated semantic boundaries, however, often fails to provide reliable trimaps even with a large unknown-region width. This results in severe artifacts in the matting results, as highlighted in the figure. We show that the natural matting method succeeds given an accurate trimap, generated using our results for demonstration.  
&ensp;&ensp;While general natural image matting is beyond the scope of our method, Figure 12 shows several examples where our method is able to generate satisfactory results on images from natural image matting datasets without requiring a trimap.  
## 4.3 Soft Color Segmentation
&ensp;&ensp;Soft color segmentation, a concept originally proposed by Tai et al. [2007], decomposes the input image into soft layers of homogeneous colors and have been shown to be useful for image editing and recoloring applications. As a conceptual comparison between semantic soft segments and soft color segments, Figure 13 shows our segments with that of unmixing-based soft color segmentation [Aksoy et al. 2017b]. For a more convenient qualitative comparison, we estimated the layer colors for our soft segments using the closedform color estimation method [Levin et al. 2008a].  
&ensp;&ensp;It is immediately visible that the content of soft color segments extend beyond the object boundaries, while our results show semantically meaningful objects in the same segment, regardless of their color content. As these representations are orthogonal to each other, they can be used in orchestration to generate targeted recoloring results.  
## 4.4 Using Semantic Soft
&ensp;&ensp;Segments for Image Editing We demonstrate several use cases of our soft segments for targeted image editing and compositing in Figure 14. Figure 14(1,3,4,7) show compositing results where we estimated the layer colors for our segments using closed-form layer color estimation [Levin et al. 2008a].  
&ensp;&ensp;Notice the natural soft transitions between the selected foreground layers and the novel background. The soft segments can also be used for targeted image edits where they are used to define masks for specific adjustment layers such as adding motion blur to the train in (2), color grading the people and the backgrounds separately in (5,6) and separate stylization of the hot-air balloon, sky, terrain and the person in (8). While these edits can be done via user-drawn masks or natural matting algorithms, our representation provides a convenient intermediate image representation to make the targeted edits effortless for the artist.  
